{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "terra_Nova.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1081817a1654253a9be8ab96e9f15f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a49bb65677d4681bae33ccb9c0f0ba6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff75d43edd9c49e48ba5c0f291930afb",
              "IPY_MODEL_4a1168a5ba7c48728c0c306fb41521e3"
            ]
          }
        },
        "2a49bb65677d4681bae33ccb9c0f0ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff75d43edd9c49e48ba5c0f291930afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_39aef76f48d04eaf8be7982f917918de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e43bcedcfea4896960e5a3d273b9df5"
          }
        },
        "4a1168a5ba7c48728c0c306fb41521e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9d17731bcd7346efb38ea5abdd6a6f7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 170M/170M [00:00&lt;00:00, 305MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d5efaf3ec5d4eccabfc3eff60dcf80d"
          }
        },
        "39aef76f48d04eaf8be7982f917918de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e43bcedcfea4896960e5a3d273b9df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9d17731bcd7346efb38ea5abdd6a6f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d5efaf3ec5d4eccabfc3eff60dcf80d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Allan-Perez/AIMisc/blob/master/terra_Nova.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G0JQ-X82yNny"
      },
      "source": [
        "# Terra Nova\n",
        "\n",
        "## <a name=\"Content\">Contents</a>\n",
        "* [Setup](#setup)\n",
        "  * [Environment](#env)\n",
        "  * [Load Configurations](#assembly)\n",
        "* [DataSet](#DataSet)\n",
        "* [Pre-Processing](#pre)\n",
        "* [Architectures](#Arch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6g-URXpF6Hrp"
      },
      "source": [
        "## <a name=\"setup\">Setup</a>\n",
        "[Return to Top](#Content)\n",
        "\n",
        "Import appropriate libraries and configure visualisation\n",
        "\n",
        "### <a name=\"env\">Environment</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zZjBkjCS3_j_",
        "outputId": "29d9b539-aa92-4959-e74a-ea7cb5e1fc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import sys\n",
        "import os\n",
        "from os import path\n",
        "from queue import *\n",
        "\n",
        "## ML imports\n",
        "import skimage.io, scipy.ndimage, scipy.interpolate, scipy.signal\n",
        "import skimage.morphology, skimage.transform, skimage.feature\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_seed = 0\n",
        "use_cuda = not False and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "learning_rate=1e-5\n",
        "\n",
        "# Allow read/write to Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "root_dir = \"/content/gdrive/Shared drives/GU Orbit\"\n",
        "base_dir = \"/content/gdrive/Shared drives/GU Orbit\"\n",
        "sys.path.insert(0, os.path.join(base_dir,\"scripts\"))\n",
        "\n",
        "from torch_assembly import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbNp2LWZ7dc4",
        "colab_type": "text"
      },
      "source": [
        "## <a name=\"assembly\">Load Configurations</a>\n",
        "[Return to Top](#Content)<br/>\n",
        "**Options**: Specifies which hyper-parameters may be changed. Maps hyper-parameter name to option (type or list)<br />\n",
        "**Configuration**: Specific set of hyper-parameter values. Maps hyper-parameter name to value (function or primitive)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHUVGoSbpOQN",
        "colab_type": "code",
        "outputId": "663a01a7-e9a3-4836-ab30-180eb8db573d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "# ToDo: Ignore any configurations that have already been trained (check models dir)\n",
        "# place\n",
        "q = Queue()\n",
        "options = Options(\"terranova\",filePath=path.join(base_dir,\"options\",\"options_terranova.pickle\"))\n",
        "Config(options=options)\n",
        "for file_name in os.listdir(path.join(base_dir,\"configurations\")):\n",
        "   print(f\"Loading Configuration: {file_name}\")\n",
        "   q.put(Config(file_path=path.join(base_dir,\"configurations\",file_name)))\n",
        "# q.get()\n",
        "config = q.get().as_dict() #Drop as_dict()?\n",
        "print(config)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'> 0\n",
            "Setting Options for all Config Instances\n",
            "Loading Configuration: configuration_default.pickle\n",
            "Loading configuration from configuration_default.pickle\n",
            "configured True <class 'bool'>\n",
            "loss torch.nn.modules.loss.BCEWithLogitsLoss <class 'str'>\n",
            "activation torch.nn.modules.activation.Sigmoid <class 'str'>\n",
            "optimizer torch.optim.Adam <class 'str'>\n",
            "no_epochs 20 <class 'int'>\n",
            "batch_size 5 <class 'int'>\n",
            "shuffle True <class 'bool'>\n",
            "architecture torchvision.models.segmentation.fcn_resnet101 <class 'str'>\n",
            "dropout_probability 0.5 <class 'float'>\n",
            "learning_rate 1e-05 <class 'float'>\n",
            "{'configured': (True, None), 'loss': ('torch.nn.modules.loss.BCEWithLogitsLoss', {}), 'activation': ('torch.nn.modules.activation.Sigmoid', {}), 'optimizer': ('torch.optim.Adam', {'lr': 1e-05}), 'no_epochs': (20, None), 'batch_size': (5, None), 'shuffle': (True, None), 'architecture': ('torchvision.models.segmentation.fcn_resnet101', {'lr': 1e-05}), 'dropout_probability': (0.5, None), 'learning_rate': (1e-05, None)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olRu9nMIzqjy",
        "colab_type": "text"
      },
      "source": [
        "## <a name=\"DataSet\">Dataset</a>\n",
        "[Return to Top](#Content)\n",
        "\n",
        "Reference DataSet Configuration\n",
        "* 18 Unique SceneIDs in DataSet\n",
        "* No. of Patches is NOT consistent between Scenes\n",
        "* PatchID + SceneID == UniqueID\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOFl7JLrTQ5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_patches = pd.read_csv(base_dir + \"/data/train/training_patches_38-Cloud.csv\",\n",
        "#                             sep=\"_LC08_\",names=[\"Patch\",\"SceneID\"],header=None,skiprows=[0]).iloc[0:8401]\n",
        "# train_patches.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S0aP_tKIEeWQ",
        "colab": {}
      },
      "source": [
        "# print(\"Number of Unique SceneIDs:\\n{}\".format(len(train_patches.SceneID.unique())))\n",
        "\n",
        "# print(\"\\nPatch Count:\")\n",
        "# patch_counts = train_patches.groupby(['SceneID'], as_index=False).count()\n",
        "# print(patch_counts.head(18))\n",
        "\n",
        "# print(\"Sum:{}\".format(patch_counts.sum(axis=0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVXvDqa-SKr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CloudDataset(Dataset):\n",
        "\n",
        "  def __init__(self, datatype = \"train\", ids=None,transforms=None):\n",
        "    \n",
        "    self.datatype = datatype;\n",
        "\n",
        "    if datatype == 'test':\n",
        "        self.data_folder = base_dir+\"/data/test\"\n",
        "    else:\n",
        "        self.data_folder = base_dir+\"/data/train\"\n",
        "    self.ids = ids # patchidx + \"_\" + sceneidx\n",
        "    self.transforms = transforms \n",
        "        \n",
        "  def __getitem__(self, ID):\n",
        "    # Given SceneIDX - a string\n",
        "    # Find all Channel Files (RGB) + NIR (Near-infrared)\n",
        "    # Combine Channels into 1 Tensor (4,W,H)\n",
        "    if type(ID) in [int,np.int32,np.int64]: # Allow indexing\n",
        "      ID = self.ids[ID]\n",
        "    print(ID,type(ID))\n",
        "    channels = []\n",
        "    for color in [\"red\",\"green\",\"blue\"]:#,\"nir\"]:\n",
        "      #print(self.data_folder+\"/\"+color+\"/\"+color + \"_\" + ID+\".TIF\")\n",
        "      channels.append(skimage.io.imread(self.data_folder+\"/\"+color+\"/\"+color+\"_\"+ID+\".TIF\"))\n",
        "    img = np.stack(channels)\n",
        "    #img = img.astype(np.int32)\n",
        "    #img = img.astype(np.int64)\n",
        "    img = img.astype(np.float32)\n",
        "\n",
        "    ground_folder = self.data_folder + \"/\"+ \"gt\"\n",
        "    #label = skimage.io.imread(ground_folder+\"/\"+\"gt_\"+ID+\".TIF\")\n",
        "    label = np.stack([skimage.io.imread(ground_folder+\"/\"+ (\"gt_\" if self.datatype == \"train\" else \"edited_corrected_gts\")+ID+\".TIF\")])\n",
        "    label = label.astype(np.float32)\n",
        "    #label.astype(np.float32)\n",
        "\n",
        "    if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "\n",
        "    return (img,label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(os.listdir(self.data_folder+\"/red/\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hy4BvW96nN_"
      },
      "source": [
        "## <a name=\"pre\">Pre-Processing</a>\n",
        "[Return to Top](#Content)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efUWgwDyz7ql",
        "colab_type": "code",
        "outputId": "45f78364-07f1-4671-8920-7bc4572bf3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "for color in [\"red\",\"green\",\"blue\",\"nir\"]:\n",
        "  train_ids = os.listdir(base_dir+\"/data/train/\"+color)\n",
        "  train_ids = [ID.strip(color+\"_\")[:-4] for ID in train_ids]\n",
        "  print(\"Training Set Size {}: {}\".format(color,len(train_ids)))\n",
        "'''\n",
        "# it's too big. Just want to try tensorboard. Not necessary rn.\n",
        "for color in [\"red\",\"green\",\"blue\",\"nir\"]:\n",
        "  test_ids = os.listdir(base_dir+\"/data/test/\"+color)\n",
        "  test_ids = [ID.strip(color+\"_\")[:-4] for ID in test_ids]\n",
        "  print(\"Testing Set Size {}: {}\".format(color,len(test_ids)))\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Size red: 8400\n",
            "Training Set Size green: 8400\n",
            "Training Set Size blue: 8400\n",
            "Training Set Size nir: 8400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor color in [\"red\",\"green\",\"blue\",\"nir\"]:\\n  test_ids = os.listdir(base_dir+\"/data/test/\"+color)\\n  test_ids = [ID.strip(color+\"_\")[:-4] for ID in test_ids]\\n  print(\"Testing Set Size {}: {}\".format(color,len(test_ids)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSXPARrSOnmd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d14fbfe5-f85a-4a9b-d5a4-854592221379"
      },
      "source": [
        "normalize = transforms.Compose([transforms.Resize(256),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean = [0.485, 0.456, 0.406], \n",
        "                                            std = [0.229, 0.224, 0.225])                                           \n",
        "])\n",
        "\n",
        "trainset = CloudDataset(datatype=\"train\",ids=train_ids,transforms=normalize)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                           batch_size=config.get(\"batch_size\",(5,None))[0],\n",
        "                                           shuffle=config.get(\"shuffle\",(True,None))[0])\n",
        "\n",
        "'''\n",
        "testset = CloudDataset(datatype=\"test\",ids=test_ids,transforms=normalize)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(testset,\n",
        "                                          batch_size=config.get(\"batch_size\",(5,None))[0],\n",
        "                                          shuffle=config.get(\"shuffle\",(True,None))[0])\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntestset = CloudDataset(datatype=\"test\",ids=test_ids,transforms=normalize)\\n\\ntest_loader = torch.utils.data.DataLoader(testset,\\n                                          batch_size=config.get(\"batch_size\",(5,None))[0],\\n                                          shuffle=config.get(\"shuffle\",(True,None))[0])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZS-EC5wL8aky"
      },
      "source": [
        "## <a name=\"Arch\">Architectures</a>\n",
        "\n",
        "### Load Architecture from Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_fw3Bqjt8Ii",
        "colab_type": "code",
        "outputId": "d54c4512-1303-4758-fd5c-2051509ea304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "config.get(\"architecture\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('torchvision.models.segmentation.fcn_resnet101', {'lr': 1e-05})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut-Texoso9Wo",
        "colab_type": "code",
        "outputId": "32bc8713-87a7-40ce-b9bf-ebaed235ed2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "a1081817a1654253a9be8ab96e9f15f9",
            "2a49bb65677d4681bae33ccb9c0f0ba6",
            "ff75d43edd9c49e48ba5c0f291930afb",
            "4a1168a5ba7c48728c0c306fb41521e3",
            "39aef76f48d04eaf8be7982f917918de",
            "6e43bcedcfea4896960e5a3d273b9df5",
            "9d17731bcd7346efb38ea5abdd6a6f7f",
            "0d5efaf3ec5d4eccabfc3eff60dcf80d"
          ]
        }
      },
      "source": [
        "architecture = config.get(\"architecture\",(\"torchvision.models.segmentation.fcn_resnet101\",None))[0]\n",
        "print(f\"Using {architecture} architecture\")\n",
        "try:\n",
        "  net = eval(f\"{architecture}(pretrained=False,num_classes=1)\")#models.segmentation.fcn_resnet101(pretrained=False,num_classes=1)\n",
        "  print(f\"Loaded {architecture} with pretrained=False and num_classes=1\")\n",
        "except:\n",
        "  net = eval(f\"{architecture}(pretrained=False\")\n",
        "  print(\"Loaded architecture does not support num_classes parameter, override the input & output layers\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using torchvision.models.segmentation.fcn_resnet101 architecture\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1081817a1654253a9be8ab96e9f15f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=178728960), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded torchvision.models.segmentation.fcn_resnet101 with pretrained=False and num_classes=1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yxfNnCjb6uvJ"
      },
      "source": [
        "**Input**: \\\\\n",
        "**3-channel RGB Images** of shape (N,3,H,W).\n",
        "\n",
        "*Where N is the Number of Images, H and W are $>=224px$*\n",
        "\n",
        "Normalization:\n",
        "* $\\mu$ = \\[0.485,0.456,0.406\\]\n",
        "* $\\sigma$ = \\[0.229,0.224,0.225\\]\n",
        "\n",
        "**Output**: \\\\\n",
        "OrderedDict with 2 Tensors of W and H as Input Tensor but with 2 classes (Cloud or No Cloud)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbjb-zVZP1pM",
        "colab_type": "text"
      },
      "source": [
        "## Load Previously Saved Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biw5Ponio9ni",
        "colab_type": "text"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmZL8BQUMslU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_segmap(image, classes=1):\n",
        "   \n",
        "  label_colors = np.array([(255, 255, 255)])\n",
        " \n",
        "  r = np.zeros_like(image).astype(np.uint8)\n",
        "  g = np.zeros_like(image).astype(np.uint8)\n",
        "  b = np.zeros_like(image).astype(np.uint8)\n",
        "   \n",
        "  for l in range(0, classes):\n",
        "    idx = image == l\n",
        "    r[idx] = label_colors[l, 0]\n",
        "    g[idx] = label_colors[l, 1]\n",
        "    b[idx] = label_colors[l, 2]\n",
        "  try:\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "  except Exception as e:\n",
        "    print(\"Axis Error r:{}g:{}b:{}\".format(r.shape,g.shape,b.shape))\n",
        "    return np.full((224,224,3),0)\n",
        "  finally:\n",
        "    passtorch.nn.MSELoss\n",
        "  return rgb\n",
        "\n",
        "def test(model, device, test_loader, loss_func=F.binary_cross_entropy_with_logits):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)['out']\n",
        "            # ToDo: Substitute loss\n",
        "            test_loss = loss_func(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            pixel_map = torch.argmax(output.squeeze(), dim=0).detach().cpu().numpy()\n",
        "            rgb_img = decode_segmap(pixel_map,classes=1)\n",
        "            plt.imshow(rgb_img); plt.show()\n",
        "\n",
        "    \n",
        "    # test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
        "        test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba3yAH9I2FXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, device, train_loader, epoch, criterion, optimizer=torch.optim.Adam):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)['out']\n",
        "        #print(output.dtype,target.dtype,output.shape,target.shape)\n",
        "        loss = criterion(output,target)\n",
        "        #loss = loss_func(output,target)\n",
        "        #loss = F.poisson_nll_loss(output, target)\n",
        "        #loss = F.mse_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    return data,output\n",
        "        \n",
        "device = \"cuda\"     \n",
        "model = net.to(device)\n",
        "optimizer, kwargs = config.get('optimizer',('torch.optim.Adam',{}))        \n",
        "optimizer = eval(f\"{optimizer}(model.parameters(),**{kwargs})\")\n",
        "loss_func, kwargs = config.get('loss',('F.binary_cross_entropy_with_logits',{}))   \n",
        "print(f\"Kwargs:{kwargs}\")   \n",
        "criterion = eval(f\"{loss_func}(**{kwargs})\")\n",
        "print(\"Loss Function: {}, Optimizer: {}\".format(config.get('loss','F.binary_cross_entropy_with_logits'),config.get(\"optimizer\",\"torch.optim.Adam\")))\n",
        "\n",
        "for epoch in range(1, config.get('no_epochs',(20,None))[0]):\n",
        "  train(model, device, train_loader, epoch, criterion, optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_7hyzCyDxKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1ac6ed8-242b-47ab-91cb-ca1729634313"
      },
      "source": [
        "## ATTEMPT TO IMPLEMENT TENSORBOARD\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def train(model, device, train_loader, epoch, criterion, optimizer=torch.optim.Adam, local_writer=None):\n",
        "    model.train()\n",
        "      \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)['out']\n",
        "        #print(output.dtype,target.dtype,output.shape,target.shape)\n",
        "        loss = criterion(output,target)\n",
        "        #loss = loss_func(output,target)\n",
        "        #loss = F.poisson_nll_loss(output, target)\n",
        "        #loss = F.mse_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            local_writer.add_scalar('Loss/train', loss.item(), epoch*batch_idx)\n",
        "    return data,output\n",
        "\n",
        "writer = SummaryWriter()\n",
        "device = \"cuda\"\n",
        "model = net.to(device)\n",
        "optimizer, kwargs = config.get('optimizer',('torch.optim.Adam',{}))        \n",
        "optimizer = eval(f\"{optimizer}(model.parameters(),**{kwargs})\")\n",
        "loss_func, kwargs = config.get('loss',('F.binary_cross_entropy_with_logits',{}))   \n",
        "print(f\"Kwargs:{kwargs}\")   \n",
        "criterion = eval(f\"{loss_func}(**{kwargs})\")\n",
        "print(\"Loss Function: {}, Optimizer: {}\".format(config.get('loss','F.binary_cross_entropy_with_logits'),config.get(\"optimizer\",\"torch.optim.Adam\")))\n",
        "\n",
        "for epoch in range(1, config.get('no_epochs',(2,None))[0]):\n",
        "  train(model, device, train_loader, epoch, criterion, optimizer=optimizer, local_writer=writer)\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kwargs:{}\n",
            "Loss Function: ('torch.nn.modules.loss.BCEWithLogitsLoss', {}), Optimizer: ('torch.optim.Adam', {'lr': 1e-05})\n",
            "patch_34_2_by_12_LC08_L1TP_064017_20160420_20170223_01_T1 <class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ac199a6431a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no_epochs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ac199a6431a4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, epoch, criterion, optimizer, local_writer)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e6a2de258c61>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ID)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Got inappropriate size arg: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BjMArw4K7rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiAdvDT80fPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.Options"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mId3n3V0SQt",
        "colab_type": "code",
        "outputId": "4ec8414e-6a44-47be-f8d1-386355f1e208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "config.get(\"batch_size\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DZ44CUTV4aY",
        "colab_type": "text"
      },
      "source": [
        "# Do not run the following code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPCIudbapBjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is for the loss evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    max_loss=float(\"-inf\")\n",
        "    min_loss=float(\"inf\")\n",
        "    total=0\n",
        "    n=0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        n+=1\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)['out']\n",
        "        #print(output.dtype,target.dtype,output.shape,target.shape)\n",
        "        loss = F.binary_cross_entropy_with_logits(output,target)\n",
        "        if max_loss < loss:\n",
        "            max_loss = loss\n",
        "        if min_loss > loss:\n",
        "            min_loss = loss\n",
        "        total += loss\n",
        "        n += 1\n",
        "        #loss = F.poisson_nll_loss(output, target)\n",
        "        #loss = F.mse_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    average_loss = total/n\n",
        "    loss_eval = (average_loss, max_loss, min_loss)\n",
        "    return data,output,loss_eval\n",
        "\n",
        "\n",
        "            \n",
        "#device = \"cpu\"     \n",
        "model = resnet.to(device)        \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "#train(model, device, train_loader, optimizer, 24)\n",
        "\n",
        "# 1. Read loss_eval of each element\n",
        "# 2. Extract the average, max, min\n",
        "# 3. Plot in Graph\n",
        "\n",
        "n_epochs = 7\n",
        "x = np.array([x for x in range(1,n_epochs)])\n",
        "losses_average = np.zeros(n_epochs)\n",
        "losses_max = np.zeros(n_epochs)\n",
        "losses_min = np.zeros(n_epochs)\n",
        "for epoch in range(1, 7):\n",
        "    out = train(model, device, train_loader, optimizer, epoch)\n",
        "    loss_eval = out[2]\n",
        "    losses_average[epoch] = loss_eval[0]\n",
        "    losses_max[epoch] = loss_eval[1]\n",
        "    losses_min[epoch] = loss_eval[2]\n",
        "\n",
        "def graph_the_losses():\n",
        "    plt.plot(x, losses_min, label = \"minimum\")\n",
        "    plt.plot(x, losses_average, label = \"average\")\n",
        "    plt.plot(x, losses_max, label = \"maximum\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legeng()\n",
        "    plt.show()\n",
        "\n",
        "graph_the_losses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84_LD_pnGlFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(model,device,test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWOW6SdU6WMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(),base_dir+\"/models/model7.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ef6KKG1H-lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(max_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hZpKYEQMn94r"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nr-dTxq381B_"
      },
      "source": [
        "**Test** with Pre-Trained Model (unrelated to cloud segmentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K09TzgU684OT"
      },
      "source": [
        "Load Image "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pec6x0qa86-P"
      },
      "source": [
        "Resize Image to be 256x256, then Centre Crop to get 224x224. Finally normalize the Tensor values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7-3E58Y49EBz"
      },
      "source": [
        "Check size of output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mq55e6iK9K8M"
      },
      "source": [
        "Extract 2D image where each pixel corresponds to a different class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lj-uex2H9j-U"
      },
      "source": [
        "Map Class to Colour in 2D image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dWdQyi2Y9ty7"
      },
      "source": [
        "View Final Segmented Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez1RVwEgnOTc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}